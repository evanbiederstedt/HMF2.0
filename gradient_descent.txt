Gradient descent: 

Optimizaiton algorithm used to find a local minimum of a function. 
Take steps proportional to the negative of the gradient of the function. 

Iterate a function f(x) at a given value until converges to a local minimum. 

Formalism:

x_{i+1} = x_{i} - \gamma f'(x_{i})

Python code (from source-which-must-not-be-cited, https://en.wikipedia.org/wiki/Gradient_descent )

# Begin with f(x) = x**4 - 3*x**3 + 2
# The derivative f'(x) is 4*x**3 - 9*x**2
#
# One finds the local minimum occurs around x = 9/4
#
#
from __future__ import print_funtion

x_old = 0
x_new = 6 # The algorithm starts at x=6
gamma = 0.01 # step size
precision = 0.00001

def f_derivative(x):
    return 4 * x**3 - 9 * x**2

while abs(x_new - x_old) > precision:
    x_old = x_new
    x_new = x_old - gamma * f_derivative(x_old)

print("Local minimum occurs at", x_new)
